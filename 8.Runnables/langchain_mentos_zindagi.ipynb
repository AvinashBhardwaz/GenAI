{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X17XNOaKGr45"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod  # Import abstract base class tools\n",
    "\n",
    "# Define an abstract base class to represent a generic Runnable component\n",
    "class Runnable(ABC):\n",
    "\n",
    "    # Abstract method that all subclasses must implement\n",
    "    # This method will define how the runnable is \"invoked\" or executed\n",
    "    @abstractmethod\n",
    "    def invoke(input_data):\n",
    "        pass  # No implementation here – forces subclasses to implement this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WPyqRUB0G2Fl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# NakliLLM is a dummy LLM class that inherits from Runnable, so it can be invoked like other LangChain components\n",
    "class NakliLLM(Runnable):\n",
    "\n",
    "    # Constructor gets called when an instance is created\n",
    "    def __init__(self):\n",
    "        print('LLM created')  # Print a message when the model is instantiated\n",
    "\n",
    "    # This method satisfies the Runnable interface — it's how LangChain will invoke this class\n",
    "    def invoke(self, prompt):\n",
    "        # A fixed list of mock responses (just like a fake LLM)\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "        # Randomly choose a response and return it in dictionary format\n",
    "        return {'response': random.choice(response_list)}\n",
    "\n",
    "    # An alternate method (non-standard) to simulate traditional LLM behavior\n",
    "    def predict(self, prompt):\n",
    "        # Same list of fake responses\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "        # Randomly return one\n",
    "        return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KDiHeGomG5ek"
   },
   "outputs": [],
   "source": [
    "# NakliPromptTemplate is a dummy prompt template class that also inherits from Runnable\n",
    "class NakliPromptTemplate(Runnable):\n",
    "\n",
    "    # Constructor to initialize the prompt template and its input variables\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = template                      # The prompt pattern (e.g., \"Write a {length} poem about {topic}\")\n",
    "        self.input_variables = input_variables        # List of expected variable names in the template\n",
    "\n",
    "    # Standard LangChain method used when chaining components together\n",
    "    def invoke(self, input_dict):\n",
    "        # Takes a dictionary of input values and fills in the template\n",
    "        return self.template.format(**input_dict)\n",
    "\n",
    "    # A custom helper method that does the same thing — not required by LangChain\n",
    "    def format(self, input_dict):\n",
    "        # Returns the formatted prompt string\n",
    "        return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aN4aoIOXMn4z"
   },
   "outputs": [],
   "source": [
    "# NakliStrOutputParser is a dummy output parser that extracts a string response from the model's output.\n",
    "# It inherits from Runnable, making it chainable in LangChain-like pipelines.\n",
    "class NakliStrOutputParser(Runnable):\n",
    "\n",
    "    # Constructor – nothing to initialize in this simple case\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # The core method used in chains – processes the input and returns the final output\n",
    "    def invoke(self, input_data):\n",
    "        # Expects input_data to be a dictionary with a 'response' key (as returned by NakliLLM)\n",
    "        return input_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IMiDtySDIIzj"
   },
   "outputs": [],
   "source": [
    "# RunnableConnector is a custom class to chain multiple Runnables together, like a mini pipeline builder.\n",
    "# It inherits from Runnable, so it fits into the LangChain-style pipeline architecture.\n",
    "\n",
    "class RunnableConnector(Runnable):\n",
    "\n",
    "    # Takes a list of Runnables (e.g., prompt template → model → output parser)\n",
    "    def __init__(self, runnable_list):\n",
    "        self.runnable_list = runnable_list\n",
    "\n",
    "    # The invoke method processes input_data through each runnable in sequence\n",
    "    def invoke(self, input_data):\n",
    "\n",
    "        # Passes the input_data through each runnable in the list\n",
    "        for runnable in self.runnable_list:\n",
    "            input_data = runnable.invoke(input_data)  # output of one becomes input for the next\n",
    "\n",
    "        # Final output after going through all Runnables\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y_c_Lk6fMJ8s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n",
      "LLM created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template for poem generation\n",
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")\n",
    "\n",
    "# Instantiate the fake LLM (simulated response generator)\n",
    "llm = NakliLLM()\n",
    "\n",
    "# Output parser that extracts just the 'response' field from the LLM output\n",
    "parser = NakliStrOutputParser()\n",
    "\n",
    "# Chain together: prompt → LLM → parser\n",
    "chain = RunnableConnector([template, llm, parser])\n",
    "\n",
    "# Run the chain with input parameters\n",
    "chain.invoke({'length': 'long', 'topic': 'india'})\n",
    "\n",
    "# --- Second example below ---\n",
    "\n",
    "# First template: ask LLM to write a joke about the given topic\n",
    "template1 = NakliPromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# Second template: ask LLM to explain the joke using the generated response\n",
    "template2 = NakliPromptTemplate(\n",
    "    template='Explain the following joke {response}',\n",
    "    input_variables=['response']\n",
    ")\n",
    "\n",
    "# LLM and parser again\n",
    "llm = NakliLLM()\n",
    "parser = NakliStrOutputParser()\n",
    "\n",
    "# Chain 1: generate a joke → gets LLM response (dict with 'response')\n",
    "chain1 = RunnableConnector([template1, llm])\n",
    "\n",
    "# Chain 2: takes the joke, explains it → goes through LLM + parser\n",
    "chain2 = RunnableConnector([template2, llm, parser])\n",
    "\n",
    "# Final chain: runs chain1, then passes its output to chain2\n",
    "final_chain = RunnableConnector([chain1, chain2])\n",
    "\n",
    "# Run final_chain with topic \"cricket\"\n",
    "final_chain.invoke({'topic': 'cricket'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
